{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa9b5240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn.models import GAT\n",
    "import os\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.data import DataEdgeAttr, DataTensorAttr\n",
    "from torch_geometric.data.storage import GlobalStorage\n",
    "import torch.serialization\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f4b12d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb25f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e626292",
   "metadata": {},
   "source": [
    "#### model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5dbe72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool\n",
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "from torch_scatter import scatter_add, scatter_mean\n",
    "import math\n",
    "\n",
    "class GlobalAttention(nn.Module):\n",
    "    \"\"\"Global Attention mechanism for DIFFormer\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim, kernel='simple', dropout=0.1):\n",
    "        super(GlobalAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel = kernel\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Projection layers for queries, keys, and values\n",
    "        self.W_q = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_k = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_v = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Kernel-specific parameters\n",
    "        if kernel == 'sigmoid':\n",
    "            self.sigma = nn.Parameter(torch.tensor(1.0))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.W_q.weight)\n",
    "        nn.init.xavier_uniform_(self.W_k.weight)\n",
    "        nn.init.xavier_uniform_(self.W_v.weight)\n",
    "        nn.init.xavier_uniform_(self.W_o.weight)\n",
    "        if hasattr(self, 'sigma'):\n",
    "            nn.init.constant_(self.sigma, 1.0)\n",
    "    \n",
    "    def forward(self, x, batch=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features [N, hidden_dim]\n",
    "            batch: Batch vector [N] indicating which graph each node belongs to\n",
    "        \"\"\"\n",
    "        N, D = x.size()\n",
    "        \n",
    "        # Compute queries, keys, values\n",
    "        Q = self.W_q(x)  # [N, hidden_dim]\n",
    "        K = self.W_k(x)  # [N, hidden_dim]\n",
    "        V = self.W_v(x)  # [N, hidden_dim]\n",
    "        \n",
    "        if batch is not None:\n",
    "            # Handle batch of graphs\n",
    "            return self._batched_attention(Q, K, V, batch)\n",
    "        else:\n",
    "            # Single graph or set of instances\n",
    "            return self._single_attention(Q, K, V)\n",
    "    \n",
    "    def _single_attention(self, Q, K, V):\n",
    "        \"\"\"Compute attention for single graph or set of instances\"\"\"\n",
    "        # Compute attention scores\n",
    "        if self.kernel == 'simple':\n",
    "            # Simple kernel: Q * K^T\n",
    "            attention_scores = torch.mm(Q, K.transpose(0, 1))\n",
    "        elif self.kernel == 'sigmoid':\n",
    "            # Sigmoid kernel: more sophisticated attention\n",
    "            attention_scores = torch.mm(Q, K.transpose(0, 1)) * self.sigma\n",
    "            attention_scores = torch.sigmoid(attention_scores)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        if self.kernel == 'simple':\n",
    "            attention_weights = F.softmax(attention_scores / math.sqrt(self.hidden_dim), dim=-1)\n",
    "        else:\n",
    "            attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Apply dropout\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Compute output\n",
    "        out = torch.mm(attention_weights, V)\n",
    "        return self.W_o(out)\n",
    "    \n",
    "    def _batched_attention(self, Q, K, V, batch):\n",
    "        \"\"\"Compute attention for batch of graphs\"\"\"\n",
    "        batch_size = batch.max().item() + 1\n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            mask = batch == i\n",
    "            q_i = Q[mask]\n",
    "            k_i = K[mask]\n",
    "            v_i = V[mask]\n",
    "            \n",
    "            if q_i.size(0) > 0:\n",
    "                out_i = self._single_attention(q_i, k_i, v_i)\n",
    "                outputs.append(out_i)\n",
    "        \n",
    "        if outputs:\n",
    "            return torch.cat(outputs, dim=0)\n",
    "        else:\n",
    "            return torch.zeros_like(Q)\n",
    "\n",
    "class DIFFormerLayer(nn.Module):\n",
    "    \"\"\"Single DIFFormer layer with global attention + GCN + residual\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim, kernel='simple', dropout=0.1, use_graph=True):\n",
    "        super(DIFFormerLayer, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_graph = use_graph\n",
    "        \n",
    "        # Global attention\n",
    "        self.global_attention = GlobalAttention(hidden_dim, kernel, dropout)\n",
    "        \n",
    "        # GCN convolution (if using graph structure)\n",
    "        if use_graph:\n",
    "            self.gcn = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index=None, batch=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features [N, hidden_dim]\n",
    "            edge_index: Edge connectivity [2, E] (optional)\n",
    "            batch: Batch vector [N] (optional)\n",
    "        \"\"\"\n",
    "        # Global attention with residual connection\n",
    "        attn_out = self.global_attention(x, batch)\n",
    "        x = self.ln1(x + self.dropout(attn_out))\n",
    "        \n",
    "        # GCN convolution (if using graph structure)\n",
    "        if self.use_graph and edge_index is not None:\n",
    "            gcn_out = self.gcn(x, edge_index)\n",
    "            x = self.ln2(x + self.dropout(gcn_out))\n",
    "        \n",
    "        # Feed-forward network with residual connection\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = x + self.dropout(ffn_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class DIFFormer_v2(nn.Module):\n",
    "    \"\"\"\n",
    "    DIFFormer v2 implementation supporting batch of graphs\n",
    "    \n",
    "    Args:\n",
    "        in_channels: Input feature dimension\n",
    "        hidden_channels: Hidden feature dimension\n",
    "        out_channels: Output feature dimension\n",
    "        num_layers: Number of DIFFormer layers\n",
    "        kernel: Attention kernel type ('simple' or 'sigmoid')\n",
    "        dropout: Dropout rate\n",
    "        use_graph: Whether to use graph structure\n",
    "        pooling: Graph-level pooling method ('mean', 'max', 'sum')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, \n",
    "                 num_layers=4, kernel='simple', dropout=0.1, \n",
    "                 use_graph=True, pooling='mean'):\n",
    "        super(DIFFormer_v2, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.use_graph = use_graph\n",
    "        self.pooling = pooling\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(in_channels, hidden_channels)\n",
    "        \n",
    "        # DIFFormer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            DIFFormerLayer(hidden_channels, kernel, dropout, use_graph)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(hidden_channels, out_channels)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.ln_final = nn.LayerNorm(hidden_channels)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.input_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.output_proj.weight)\n",
    "        nn.init.zeros_(self.input_proj.bias)\n",
    "        nn.init.zeros_(self.output_proj.bias)\n",
    "    \n",
    "    def forward(self, x, edge_index=None, batch=None, n_nodes=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features [N, in_channels]\n",
    "            edge_index: Edge connectivity [2, E] (optional)\n",
    "            batch: Batch vector [N] indicating graph membership (optional)\n",
    "            n_nodes: Number of nodes per graph [batch_size] (optional)\n",
    "        \n",
    "        Returns:\n",
    "            out: Node embeddings [N, out_channels] or graph embeddings [batch_size, out_channels]\n",
    "        \"\"\"\n",
    "        # Input projection\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # Apply DIFFormer layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index, batch)\n",
    "        \n",
    "        # Final layer normalization\n",
    "        x = self.ln_final(x)\n",
    "        \n",
    "        # Output projection\n",
    "        out = self.output_proj(x)\n",
    "        \n",
    "        # Graph-level pooling if batch is provided\n",
    "        if batch is not None and self.pooling is not None:\n",
    "            if self.pooling == 'mean':\n",
    "                out = global_mean_pool(out, batch)\n",
    "            elif self.pooling == 'max':\n",
    "                out = global_max_pool(out, batch)\n",
    "            elif self.pooling == 'sum':\n",
    "                out = scatter_add(out, batch, dim=0)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_attention_weights(self, x, edge_index=None, batch=None, layer_idx=0):\n",
    "        \"\"\"Get attention weights from a specific layer\"\"\"\n",
    "        # Forward pass up to the specified layer\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i == layer_idx:\n",
    "                # Get attention weights from this layer\n",
    "                Q = layer.global_attention.W_q(x)\n",
    "                K = layer.global_attention.W_k(x)\n",
    "                \n",
    "                if layer.global_attention.kernel == 'simple':\n",
    "                    attention_scores = torch.mm(Q, K.transpose(0, 1))\n",
    "                    attention_weights = F.softmax(attention_scores / math.sqrt(self.hidden_channels), dim=-1)\n",
    "                else:\n",
    "                    attention_scores = torch.mm(Q, K.transpose(0, 1)) * layer.global_attention.sigma\n",
    "                    attention_scores = torch.sigmoid(attention_scores)\n",
    "                    attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "                \n",
    "                return attention_weights\n",
    "            else:\n",
    "                x = layer(x, edge_index, batch)\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Example usage and utility functions\n",
    "def create_difformer_model(task_type='node_classification', **kwargs):\n",
    "    \"\"\"\n",
    "    Factory function to create DIFFormer models for different tasks\n",
    "    \n",
    "    Args:\n",
    "        task_type: 'node_classification', 'graph_classification', 'regression'\n",
    "        **kwargs: Additional arguments for DIFFormer_v2\n",
    "    \"\"\"\n",
    "    if task_type == 'node_classification':\n",
    "        return DIFFormer_v2(pooling=None, **kwargs)\n",
    "    elif task_type == 'graph_classification':\n",
    "        return DIFFormer_v2(pooling='mean', **kwargs)\n",
    "    elif task_type == 'regression':\n",
    "        return DIFFormer_v2(pooling='mean', **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task type: {task_type}\")\n",
    "\n",
    "# # Example instantiation\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example: Node classification\n",
    "#     model = DIFFormer_v2(\n",
    "#         in_channels=128,\n",
    "#         hidden_channels=256,\n",
    "    #     out_channels=64,\n",
    "    #     num_layers=4,\n",
    "    #     kernel='simple',\n",
    "    #     dropout=0.1,\n",
    "    #     use_graph=True,\n",
    "    #     pooling=None  # None for node-level tasks\n",
    "    # )\n",
    "    \n",
    "    # print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "    \n",
    "    # # Example input\n",
    "    # x = torch.randn(100, 128)  # 100 nodes, 128 features\n",
    "    # edge_index = torch.randint(0, 100, (2, 200))  # 200 edges\n",
    "    # batch = torch.zeros(100, dtype=torch.long)  # Single graph\n",
    "    \n",
    "    # # Forward pass\n",
    "    # output = model(x, edge_index, batch)\n",
    "    # print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "    # # Get attention weights\n",
    "    # attention_weights = model.get_attention_weights(x, edge_index, batch, layer_idx=0)\n",
    "    # print(f\"Attention weights shape: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51bd02bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb360b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91e55f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data \n",
    "with torch.serialization.safe_globals([Data]):\n",
    "    data_list = torch.load(\"training_data\\Datacheckpoint_latest_22\", map_location='cuda', weights_only=False)\n",
    "\n",
    "labels = json.load(open(\"label_encoding.json\"))\n",
    "batch_size = 1\n",
    "\n",
    "train_split = int(len(data_list) * 0.8)\n",
    "train_data = data_list[:train_split]\n",
    "val_data = data_list[train_split:]\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "in_channels = data_list[0].x.size(1)\n",
    "# in_channels =18\n",
    "num_classes = len(labels)\n",
    "\n",
    "model_dir = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\GAT-model testing\\\\GAT-test\\\\models\\\\model_diffusion\"\n",
    "results_dir = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\GAT-model testing\\\\GAT-test\\\\results\"\n",
    "plots_dir = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\GAT-model testing\\\\GAT-test\\\\plots\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(plots_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95c311a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    config = {\n",
    "         # ----- Suggest Hyperparameters -----\n",
    "    'hidden_dim' : trial.suggest_categorical(\"hidden_channels\", [128, 256, 512]),\n",
    "    'num_layers' : trial.suggest_int(\"num_layers\", 2, 6),\n",
    "    'dropout' : trial.suggest_float(\"dropout\", 0.1, 0.5),\n",
    "    'lr' : trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True),\n",
    "    'use_graph' : trial.suggest_categorical(\"use_graph\", [True, False])\n",
    "    }\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"trial_{trial.number}\", nested=True):\n",
    "        mlflow.log_params(config)\n",
    "        mlflow.set_tag(\"cv_strategy\", \"KFold\")\n",
    "        mlflow.set_tag(\"model\", \"GATv2\")\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        data_indices = list(range(len(data_list)))\n",
    "        fold_val_acc = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(data_indices)):\n",
    "            train_data = [data_list[i] for i in train_idx]\n",
    "            val_data = [data_list[i] for i in val_idx]\n",
    "\n",
    "            train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "            model = DIFFormer_v2(\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels=256,\n",
    "            out_channels=num_classes,\n",
    "            num_layers=4,\n",
    "            kernel='simple',\n",
    "            dropout=0.1,\n",
    "            use_graph=True,\n",
    "            pooling=None  # None for node-level tasks\n",
    "            ).to(device)\n",
    "\n",
    "            all_train_labels = torch.cat([data.y for data in train_loader.dataset])\n",
    "            class_weights = 1.0 / (torch.bincount(all_train_labels, minlength=num_classes).float() + 1e-6)\n",
    "            class_weights = class_weights / class_weights.sum()\n",
    "            class_weights = class_weights.to(device)\n",
    "\n",
    "            criterion = CrossEntropyLoss(weight=class_weights)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=5e-4)\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "            best_val_acc = 0\n",
    "            best_val_loss = float('inf')\n",
    "            wait = 0\n",
    "            patience = 30\n",
    "            min_delta = 1e-4\n",
    "\n",
    "            training_loss, validation_loss, validation_acc = [], [], []\n",
    "\n",
    "            for epoch in range(501):\n",
    "                model.train()\n",
    "                total_loss = 0\n",
    "                correct_train = 0\n",
    "                total_train = 0\n",
    "\n",
    "                for data in train_loader:\n",
    "                    data = data.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    out = model(data.x, data.edge_index)\n",
    "                    loss = criterion(out, data.y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "                    correct_train += (out.argmax(dim=1) == data.y).sum().item()\n",
    "                    total_train += data.y.size(0)\n",
    "\n",
    "                avg_train_loss = total_loss / len(train_loader)\n",
    "                train_acc = correct_train / total_train\n",
    "                training_loss.append(avg_train_loss)\n",
    "\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                correct, total = 0, 0\n",
    "                with torch.no_grad():\n",
    "                    for data in val_loader:\n",
    "                        data = data.to(device)\n",
    "                        out = model(data.x, data.edge_index)\n",
    "                        loss = criterion(out, data.y)\n",
    "                        val_loss += loss.item()\n",
    "                        correct += (out.argmax(dim=1) == data.y).sum().item()\n",
    "                        total += data.y.size(0)\n",
    "\n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "                val_acc = correct / total\n",
    "                validation_loss.append(avg_val_loss)\n",
    "                validation_acc.append(val_acc)\n",
    "\n",
    "                # Log metrics per epoch\n",
    "                mlflow.log_metric(f\"fold{fold+1}_train_loss\", avg_train_loss, step=epoch)\n",
    "                mlflow.log_metric(f\"fold{fold+1}_val_loss\", avg_val_loss, step=epoch)\n",
    "                mlflow.log_metric(f\"fold{fold+1}_val_acc\", val_acc, step=epoch)\n",
    "                mlflow.log_metric(f\"fold{fold+1}_train_acc\", train_acc, step=epoch)\n",
    "\n",
    "                scheduler.step(avg_val_loss)\n",
    "                trial.report(val_acc, epoch)\n",
    "\n",
    "                print(f\"Epoch {epoch+1:03d} | Fold {fold+1} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Train loss: {avg_train_loss:.4f} Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "                if avg_val_loss < best_val_loss - min_delta:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    best_val_acc = val_acc\n",
    "                    wait = 0\n",
    "                else:\n",
    "                    wait += 1\n",
    "                    if wait >= patience:\n",
    "                        print(f\" Early stopping at epoch {epoch+1} for fold {fold+1}\")\n",
    "                        break\n",
    "\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "            print(f\" Fold {fold+1}: Best Val Acc = {best_val_acc:.4f}\")\n",
    "            fold_val_acc.append(float(best_val_acc))\n",
    "\n",
    "        mean_val_acc = float(np.mean(fold_val_acc))\n",
    "        mlflow.log_metric(\"mean_val_acc\", mean_val_acc)\n",
    "\n",
    "        return mean_val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a6aaaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 12:02:16,641] A new study created in memory with name: no-name-7bfb48dd-ac15-4fc4-9e5a-ba730521da7a\n",
      "[W 2025-07-16 12:02:21,359] Trial 0 failed with parameters: {'hidden_channels': 512, 'num_layers': 3, 'dropout': 0.3488230276100335, 'lr': 0.0001518084130835547, 'use_graph': False} because of the following error: TypeError(\"DIFFormer_v2.forward() got an unexpected keyword argument 'edge_weight'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_35248\\452603071.py\", line 89, in objective\n",
      "    out = model(data.x, data.edge_index, edge_weight=data.edge_attr)\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: DIFFormer_v2.forward() got an unexpected keyword argument 'edge_weight'\n",
      "[W 2025-07-16 12:02:21,380] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run trial_0 at: http://127.0.0.1:5000/#/experiments/128168505728161317/runs/1b85a257d37146f39c819d0cbda5a0f9\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/128168505728161317\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DIFFormer_v2.forward() got an unexpected keyword argument 'edge_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Create and run the Optuna study\u001b[39;00m\n\u001b[0;32m      8\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mvalue)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\optuna\\study\\study.py:489\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    389\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 64\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    249\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    252\u001b[0m ):\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[6], line 89\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m     88\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 89\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m     91\u001b[0m     val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mTypeError\u001b[0m: DIFFormer_v2.forward() got an unexpected keyword argument 'edge_weight'"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Set MLflow experiment name â€” all trials/logs will be grouped under this\n",
    "mlflow.set_experiment(\"DIFFormer_v2_Optuna_Experiment_1\")\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\") \n",
    "\n",
    "# Create and run the Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=70)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(\"Accuracy:\", study.best_trial.value)\n",
    "print(\"Params:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7670bbc",
   "metadata": {},
   "source": [
    "### final_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f448ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# ----------------- SET MLFLOW CONFIG ----------------------\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"GATv2_Final_Training\")\n",
    "\n",
    "# ----------------- LOAD BEST HYPERPARAMETERS ----------------------\n",
    "best_params_path = os.path.join(model_dir, \"best_params.json\")\n",
    "best_params = study.best_trial.params\n",
    "with open(best_params_path, \"w\") as f:\n",
    "    json.dump(best_params, f, indent=4)\n",
    "\n",
    "# ----------------- SETUP FINAL TRAINING ----------------------\n",
    "final_model = GAT(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=best_params['hidden_channels'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    out_channels=num_classes,\n",
    "    dropout=best_params['dropout'],\n",
    "    heads=best_params['heads'],\n",
    "    v2=True,\n",
    "    edge_dim=1,\n",
    "    jk='lstm'\n",
    ").to(device)\n",
    "\n",
    "final_model = DIFFormer_v2(\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels=best_params['hidden_channels'],\n",
    "            out_channels=num_classes,\n",
    "            num_layers=best_params['num_layers'],\n",
    "            kernel='simple',\n",
    "            dropout=0.1,\n",
    "            use_graph=True,\n",
    "            pooling=None  # None for node-level tasks\n",
    "            ).to(device)\n",
    "\n",
    "# Class-weighted loss\n",
    "all_labels = torch.cat([data.y for data in train_loader.dataset])\n",
    "class_counts = torch.bincount(all_labels, minlength=num_classes)\n",
    "class_weights = 1.0 / (class_counts.float() + 1e-6)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "criterion = CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(final_model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = os.path.join(model_dir, \"GAT_full_model_best_001.pt\")\n",
    "\n",
    "# ----------------- TRAINING LOOP ----------------------\n",
    "for epoch in range(500):\n",
    "    final_model.train()\n",
    "    train_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = final_model(data.x, data.edge_index, edge_weight=data.edge_attr)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    final_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            data = data.to(device)\n",
    "            out = final_model(data.x, data.edge_index, edge_weight=data.edge_attr)\n",
    "            loss = criterion(out, data.y)\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Save best weights\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(final_model.state_dict(), best_model_path)\n",
    "\n",
    "    print(f\"[FINAL TRAIN] Epoch {epoch+1:03d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Save full model (.pt)\n",
    "full_model_path = os.path.join(model_dir, \"GAT_final_model.pt\")\n",
    "torch.save(final_model, full_model_path)\n",
    "print(f\"Full model saved to {full_model_path}\")\n",
    "print(f\" Best model (lowest val loss) saved to {best_model_path}\")\n",
    "\n",
    "# ----------------- PLOT LOSS CURVE ----------------------\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Final Model Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plot_path = os.path.join(plots_dir, \"final_train_val_loss.png\")\n",
    "plt.savefig(plot_path)\n",
    "plt.show()\n",
    "\n",
    "# ----------------- MLFLOW LOGGING ----------------------\n",
    "# Load best weights before logging\n",
    "final_model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "with mlflow.start_run(run_name=\"final_retrain\"):\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_artifact(best_params_path)\n",
    "    mlflow.log_artifact(plot_path)\n",
    "\n",
    "    for epoch, (train_l, val_l) in enumerate(zip(train_losses, val_losses)):\n",
    "        mlflow.log_metric(\"train_loss\", train_l, step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", val_l, step=epoch)\n",
    "\n",
    "    # âœ… Log model\n",
    "    mlflow.pytorch.log_model(final_model, artifact_path=\"final_model\")\n",
    "    print(\" Final model logged to MLflow.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENVGAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
