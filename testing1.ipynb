{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f67970e",
   "metadata": {},
   "source": [
    "### loading the data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15e1c64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2157a001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Using cached torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "Collecting aiohttp (from torch_geometric)\n",
      "  Downloading aiohttp-3.12.13-cp310-cp310-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting fsspec (from torch_geometric)\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from torch_geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from torch_geometric) (2.0.1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from torch_geometric) (7.0.0)\n",
      "Collecting pyparsing (from torch_geometric)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from torch_geometric) (2.32.4)\n",
      "Collecting tqdm (from torch_geometric)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->torch_geometric)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch_geometric)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->torch_geometric)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->torch_geometric)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch_geometric)\n",
      "  Downloading frozenlist-1.7.0-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch_geometric)\n",
      "  Downloading multidict-6.6.3-cp310-cp310-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->torch_geometric)\n",
      "  Downloading propcache-0.3.2-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->torch_geometric)\n",
      "  Downloading yarl-1.20.1-cp310-cp310-win_amd64.whl.metadata (76 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->torch_geometric) (3.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from requests->torch_geometric) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from requests->torch_geometric) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from requests->torch_geometric) (2025.6.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from tqdm->torch_geometric) (0.4.6)\n",
      "Using cached torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "Downloading aiohttp-3.12.13-cp310-cp310-win_amd64.whl (450 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading multidict-6.6.3-cp310-cp310-win_amd64.whl (45 kB)\n",
      "Downloading yarl-1.20.1-cp310-cp310-win_amd64.whl (86 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.7.0-cp310-cp310-win_amd64.whl (43 kB)\n",
      "Downloading propcache-0.3.2-cp310-cp310-win_amd64.whl (41 kB)\n",
      "Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, pyparsing, propcache, multidict, fsspec, frozenlist, attrs, async-timeout, aiohappyeyeballs, yarl, aiosignal, aiohttp, torch_geometric\n",
      "\n",
      "   ----------------------------------------  0/13 [tqdm]\n",
      "   --- ------------------------------------  1/13 [pyparsing]\n",
      "   --------- ------------------------------  3/13 [multidict]\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "   ------------------ ---------------------  6/13 [attrs]\n",
      "   ------------------ ---------------------  6/13 [attrs]\n",
      "   ------------------------ ---------------  8/13 [aiohappyeyeballs]\n",
      "   ------------------------------ --------- 10/13 [aiosignal]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ---------------------------------------- 13/13 [torch_geometric]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.4.0 async-timeout-5.0.1 attrs-25.3.0 frozenlist-1.7.0 fsspec-2025.5.1 multidict-6.6.3 propcache-0.3.2 pyparsing-3.2.3 torch_geometric-2.6.1 tqdm-4.67.1 yarl-1.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.5.1 requires sympy==1.13.1, but you have sympy 1.13.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (3.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric\n",
    "# !pip install torch\n",
    "!pip install networkx\n",
    "# !pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f14fdc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 0], Text=[480], ValueType=[480], EndsWithColon=[480], left_spacing=[480], right_spacing=[480], IsHorizontalNeighbourKey=[480], IsVerticalNeighbourKey=[480], Label=[480], num_nodes=480)\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Load the .graphml file\n",
    "G_nx = nx.read_graphml(\"all_documents_newww.graphml\")\n",
    "\n",
    "# Optional: Convert node attributes to float tensors (if needed)\n",
    "for node_id in G_nx.nodes:\n",
    "    attrs = G_nx.nodes[node_id]\n",
    "    for k, v in attrs.items():\n",
    "        try:\n",
    "            G_nx.nodes[node_id][k] = float(v)\n",
    "        except:\n",
    "            pass  # Skip non-numeric attributes\n",
    "\n",
    "# Convert to PyTorch Geometric format\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "data = from_networkx(G_nx)\n",
    "\n",
    "# Now data is ready to be used with GAT\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0945e964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_nodes = data.num_nodes\n",
    "data.x = torch.eye(num_nodes)  # One-hot features\n",
    "print(data.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eab48052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([480, 480])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b842da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Saved as graph_data.pt\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Step 1: Load .graphml file\n",
    "G = nx.read_graphml(\"all_documents_newww.graphml\")\n",
    "\n",
    "# Optional: convert node attributes to float (if needed)\n",
    "for node in G.nodes:\n",
    "    for key, val in G.nodes[node].items():\n",
    "        try:\n",
    "            G.nodes[node][key] = float(val)\n",
    "        except:\n",
    "            pass  # Skip non-numeric attributes\n",
    "\n",
    "# Step 2: Convert to PyTorch Geometric Data\n",
    "data = from_networkx(G)\n",
    "\n",
    "# If node features are missing, create identity or random features\n",
    "if not hasattr(data, 'x'):\n",
    "    num_nodes = data.num_nodes\n",
    "    data.x = torch.eye(num_nodes)  # one-hot as fallback\n",
    "    # Or use: data.x = torch.rand(num_nodes, feature_dim)\n",
    "# print(data.x)\n",
    "# Step 3: Save to .pt file\n",
    "torch.save(data, \"graph_data.pt\")\n",
    "print(\"Saved as graph_data.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc3382d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "data = torch.load(\"graph_data.pt\",weights_only=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b956e69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], size=(2, 0), dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "# print(data)\n",
    "print(data.edge_index)\n",
    "# there is no edge index in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4f513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0, 0, 0, 0, 0, 0, 0, 1, 0]'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.ValueType[0] # alphanumeric, # numeric etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4139a815",
   "metadata": {},
   "source": [
    "### model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05c2ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn.models import GAT\n",
    "import os\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.data import DataEdgeAttr, DataTensorAttr\n",
    "from torch_geometric.data.storage import GlobalStorage\n",
    "import torch.serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d42ed365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "12.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e339bb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
      "Collecting torch-scatter\n",
      "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_scatter-2.1.2%2Bpt25cu121-cp310-cp310-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 3.5/3.5 MB 52.4 MB/s eta 0:00:00\n",
      "Installing collected packages: torch-scatter\n",
      "Successfully installed torch-scatter-2.1.2+pt25cu121\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.5.1+cu121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93779f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # âœ… NodeFormer-style Graph Transformer for Node-Level Classification\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.nn import knn_graph\n",
    "# from torch_scatter import scatter_mean\n",
    "\n",
    "# class NodeFormerLayer(nn.Module):\n",
    "#     def __init__(self, in_dim, out_dim, k=16):\n",
    "#         super().__init__()\n",
    "#         self.k = k\n",
    "#         self.attn_proj = nn.Linear(in_dim, out_dim)\n",
    "#         self.val_proj = nn.Linear(in_dim, out_dim)\n",
    "#         self.out_proj = nn.Linear(out_dim, out_dim)\n",
    "\n",
    "#     def forward(self, x, batch):\n",
    "#         # x: [N, F]  -- node features\n",
    "#         # batch: [N] -- batch IDs\n",
    "\n",
    "#         edge_index = knn_graph(x, self.k, batch=batch, loop=False)\n",
    "#         row, col = edge_index\n",
    "\n",
    "#         # Attention score between i and j\n",
    "#         q = self.attn_proj(x)  # [N, D]\n",
    "#         v = self.val_proj(x)\n",
    "\n",
    "#         attn_score = (q[row] * q[col]).sum(dim=-1) / (q.size(-1) ** 0.5)  # [E]\n",
    "#         attn_score = F.softmax(attn_score, dim=0)\n",
    "\n",
    "#         # Weighted aggregation\n",
    "#         out = attn_score.unsqueeze(-1) * v[col]  # [E, D]\n",
    "#         out = scatter_mean(out, row, dim=0, dim_size=x.size(0))  # [N, D]\n",
    "\n",
    "#         return self.out_proj(out) + x  # Residual\n",
    "\n",
    "\n",
    "# class NodeFormer(nn.Module):\n",
    "#     def __init__(self, in_dim, hidden_dim, out_dim, num_layers=2, k=16):\n",
    "#         super().__init__()\n",
    "#         self.input_proj = nn.Linear(in_dim, hidden_dim)\n",
    "#         self.layers = nn.ModuleList([\n",
    "#             NodeFormerLayer(hidden_dim, hidden_dim, k=k)\n",
    "#             for _ in range(num_layers)\n",
    "#         ])\n",
    "#         self.classifier = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "#     def forward(self, x, batch):\n",
    "#         x = self.input_proj(x)\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x, batch)\n",
    "#         return self.classifier(x)\n",
    "\n",
    "\n",
    "# # # Example usage:\n",
    "# # if __name__ == '__main__':\n",
    "# #     from torch_geometric.datasets import Planetoid\n",
    "# #     from torch_geometric.loader import DataLoader\n",
    "# #     from torch_geometric.utils import to_dense_batch\n",
    "    \n",
    "# #     dataset = Planetoid(root=\"./data\", name=\"Cora\")\n",
    "# #     data = dataset[0]\n",
    "\n",
    "# #     model = NodeFormer(in_dim=dataset.num_node_features, hidden_dim=64, out_dim=dataset.num_classes)\n",
    "# #     optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "# #     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# #     model.train()\n",
    "# #     for epoch in range(100):\n",
    "# #         optimizer.zero_grad()\n",
    "# #         out = model(data.x, batch=torch.zeros_like(data.y))\n",
    "# #         loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "# #         loss.backward()\n",
    "# #         optimizer.step()\n",
    "# #         print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50b4cc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smooth_curve(data, weight=0.9):\n",
    "    smoothed = []\n",
    "    last = data[0]\n",
    "    for point in data:\n",
    "        smoothed_val = last * weight + (1 - weight) * point\n",
    "        smoothed.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    return smoothed\n",
    "\n",
    "\n",
    "def train_single_config(config, train_loader, val_loader, in_channels, num_classes, run_name, model_dir, results_dir, plots_dir):\n",
    "    model = GAT(\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels=config['hidden_channels'],\n",
    "        num_layers=config['num_layers'],\n",
    "        out_channels=num_classes,\n",
    "        dropout=config['dropout'],\n",
    "        heads=config['heads'],\n",
    "        v2=True,\n",
    "        edge_dim=1,\n",
    "        jk='lstm'\n",
    "    )\n",
    "    # model = NodeFormer(in_dim=in_channels, hidden_dim=64, out_dim=4)\n",
    "\n",
    "    all_labels = torch.cat([data.y for data in train_loader.dataset])\n",
    "    class_counts = torch.bincount(all_labels, minlength=num_classes)\n",
    "    class_weights = 1.0 / (class_counts.float() + 1e-6)\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "    criterion = CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "    training_loss, validation_loss, validation_acc = [], [], []\n",
    "\n",
    "    for epoch in range(500):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index, edge_weight=data.edge_attr)\n",
    "            # model = NodeFormer(in_dim=data.x, hidden_dim=64, out_dim=4)\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        training_loss.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                out = model(data.x, data.edge_index, edge_weight=data.edge_attr)\n",
    "                loss = criterion(out, data.y)\n",
    "                val_total_loss += loss.item()\n",
    "                pred = out.argmax(dim=1)\n",
    "                correct += (pred == data.y).sum().item()\n",
    "                total += data.y.size(0)\n",
    "\n",
    "        avg_val_loss = val_total_loss / len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "        validation_loss.append(avg_val_loss)\n",
    "        validation_acc.append(val_accuracy)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1:03d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "    model_path = os.path.join(model_dir, f\"{run_name}.pth\")\n",
    "    csv_path = os.path.join(results_dir, f\"{run_name}.csv\")\n",
    "    plot_path = os.path.join(plots_dir, f\"{run_name}.png\")\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Epoch': list(range(1, len(training_loss)+1)),\n",
    "        'TrainLoss': training_loss,\n",
    "        'ValLoss': validation_loss,\n",
    "        'ValAcc': validation_acc\n",
    "    })\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(smooth_curve(training_loss), label='Train')\n",
    "    plt.plot(validation_loss, label='Val')\n",
    "    plt.title(run_name)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b012795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Using cached optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.16.3-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Using cached colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from optuna) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from optuna) (25.0)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
      "  Downloading sqlalchemy-2.0.41-cp310-cp310-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Using cached mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: tomli in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from alembic>=1.5.0->optuna) (2.0.1)\n",
      "Collecting greenlet>=1 (from sqlalchemy>=1.4.2->optuna)\n",
      "  Downloading greenlet-3.2.3-cp310-cp310-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Using cached optuna-4.4.0-py3-none-any.whl (395 kB)\n",
      "Downloading alembic-1.16.3-py3-none-any.whl (246 kB)\n",
      "Downloading sqlalchemy-2.0.41-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 9.8 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.2.3-cp310-cp310-win_amd64.whl (296 kB)\n",
      "Using cached colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Using cached mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: Mako, greenlet, colorlog, sqlalchemy, alembic, optuna\n",
      "\n",
      "   ---------------------------------------- 0/6 [Mako]\n",
      "   ------ --------------------------------- 1/6 [greenlet]\n",
      "   ------------- -------------------------- 2/6 [colorlog]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------------- ------------- 4/6 [alembic]\n",
      "   -------------------------- ------------- 4/6 [alembic]\n",
      "   -------------------------- ------------- 4/6 [alembic]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   ---------------------------------------- 6/6 [optuna]\n",
      "\n",
      "Successfully installed Mako-1.3.10 alembic-1.16.3 colorlog-6.9.0 greenlet-3.2.3 optuna-4.4.0 sqlalchemy-2.0.41\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8895ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d08e2a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[103, 18], edge_index=[2, 79], edge_attr=[79, 1], y=[103]),\n",
       " Data(x=[111, 18], edge_index=[2, 84], edge_attr=[84, 1], y=[111]),\n",
       " Data(x=[35, 18], edge_index=[2, 25], edge_attr=[25, 1], y=[35]),\n",
       " Data(x=[76, 18], edge_index=[2, 56], edge_attr=[56, 1], y=[76]),\n",
       " Data(x=[256, 18], edge_index=[2, 194], edge_attr=[194, 1], y=[256]),\n",
       " Data(x=[48, 18], edge_index=[2, 36], edge_attr=[36, 1], y=[48]),\n",
       " Data(x=[92, 18], edge_index=[2, 70], edge_attr=[70, 1], y=[92]),\n",
       " Data(x=[138, 18], edge_index=[2, 106], edge_attr=[106, 1], y=[138]),\n",
       " Data(x=[40, 18], edge_index=[2, 30], edge_attr=[30, 1], y=[40]),\n",
       " Data(x=[81, 18], edge_index=[2, 60], edge_attr=[60, 1], y=[81]),\n",
       " Data(x=[113, 18], edge_index=[2, 86], edge_attr=[86, 1], y=[113]),\n",
       " Data(x=[185, 18], edge_index=[2, 143], edge_attr=[143, 1], y=[185]),\n",
       " Data(x=[85, 18], edge_index=[2, 64], edge_attr=[64, 1], y=[85]),\n",
       " Data(x=[73, 18], edge_index=[2, 56], edge_attr=[56, 1], y=[73]),\n",
       " Data(x=[54, 18], edge_index=[2, 41], edge_attr=[41, 1], y=[54]),\n",
       " Data(x=[176, 18], edge_index=[2, 136], edge_attr=[136, 1], y=[176])]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = torch.load(\"datacheckpoint_training_(15).pt\", map_location='cuda', weights_only=False)\n",
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1857383e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list[0].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed65b30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# from your_model_file import GAT  # Replace with actual import\n",
    "# from your_utils import smooth_curve  # Replace if defined elsewhere\n",
    "\n",
    "def smooth_curve(data, weight=0.9):\n",
    "    smoothed = []\n",
    "    last = data[0]\n",
    "    for point in data:\n",
    "        smoothed_val = last * weight + (1 - weight) * point\n",
    "        smoothed.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    return smoothed\n",
    "\n",
    "\n",
    "\n",
    "# Load data \n",
    "with torch.serialization.safe_globals([Data]):\n",
    "    data_list = torch.load(\"training_data\\Datacheckpoint_latest_22\", map_location='cuda', weights_only=False)\n",
    "\n",
    "labels = json.load(open(\"label_encoding.json\"))\n",
    "batch_size = 1\n",
    "\n",
    "train_split = int(len(data_list) * 0.8)\n",
    "train_data = data_list[:train_split]\n",
    "val_data = data_list[train_split:]\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "in_channels = data_list[0].x.size(1)\n",
    "# in_channels =18\n",
    "num_classes = len(labels)\n",
    "\n",
    "model_dir = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\GAT-model testing\\\\GAT-test\\\\models\\\\model\"\n",
    "results_dir = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\GAT-model testing\\\\GAT-test\\\\results\"\n",
    "plots_dir = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\GAT-model testing\\\\GAT-test\\\\plots\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "def objective(trial):\n",
    "    config = {\n",
    "        'hidden_channels': trial.suggest_categorical('hidden_channels', [64, 128, 256]),\n",
    "        'num_layers': trial.suggest_int('num_layers', 1, 3),\n",
    "        'heads': trial.suggest_categorical('heads', [1, 2, 4, 8]),\n",
    "        'dropout': trial.suggest_float('dropout', 0.0, 0.5),\n",
    "        'hidden_dim': trial.suggest_categorical('hidden_dim', [64, 128, 256])\n",
    "    }\n",
    "\n",
    "    model = GAT(\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels=config['hidden_channels'],\n",
    "        num_layers=config['num_layers'],\n",
    "        out_channels=num_classes,\n",
    "        dropout=config['dropout'],\n",
    "        heads=config['heads'],\n",
    "        v2=True,\n",
    "        edge_dim=1,\n",
    "        jk='lstm'\n",
    "    ).to(device)  # ðŸš€ Move model to GPU\n",
    "    # model = NodeFormer(in_dim=in_channels, hidden_dim=64, out_dim=4).to(device)\n",
    "\n",
    "    all_labels = torch.cat([data.y for data in train_loader.dataset])\n",
    "    class_counts = torch.bincount(all_labels, minlength=num_classes)\n",
    "    class_weights = 1.0 / (class_counts.float() + 1e-6)\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "    class_weights = class_weights.to(device)  # ðŸŽ¯ Move weights to GPU\n",
    "\n",
    "    criterion = CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    training_loss, validation_loss, validation_acc, training_acc = [], [], [], []\n",
    "    best_model_state_path = None\n",
    "    best_model_full_path = None\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = 30  # Number of epochs to wait for improvement\n",
    "    min_delta = 1e-4  # Minimum change to qualify as improvement\n",
    "    wait = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(500):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)  #  Move batch to GPU\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index, edge_weight=data.edge_attr)\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pred_train = out.argmax(dim=1)\n",
    "            correct_train += (pred_train == data.y).sum().item()\n",
    "            total_train += data.y.size(0)\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        training_loss.append(avg_train_loss)\n",
    "        train_acc = correct_train / total_train if total_train > 0 else 0\n",
    "        training_acc.append(train_acc)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                data = data.to(device)  # ðŸš€ Move validation data to GPU\n",
    "                out = model(data.x, data.edge_index, edge_weight=data.edge_attr)\n",
    "                loss = criterion(out, data.y)\n",
    "                val_loss += loss.item()\n",
    "                pred = out.argmax(dim=1)\n",
    "                correct += (pred == data.y).sum().item()\n",
    "                total += data.y.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = correct / total if total > 0 else 0\n",
    "        validation_loss.append(avg_val_loss)\n",
    "        validation_acc.append(val_acc)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "        print(f\"Epoch {epoch + 1:03d} | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        trial.report(val_acc, epoch)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if avg_val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = avg_val_loss\n",
    "            wait = 0\n",
    "            # Save best model at this point\n",
    "            run_name = f\"BestTrial_H{config['hidden_channels']}_L{config['num_layers']}_HD{config['heads']}_DO{int(config['dropout']*10)}\"\n",
    "            best_model_state_path = os.path.join(model_dir, f\"{run_name}_best_state_dict.pth\")\n",
    "            best_model_full_path = os.path.join(model_dir, f\"{run_name}_best_full.pt\")\n",
    "            torch.save(model.state_dict(), best_model_state_path)\n",
    "            torch.save(model, best_model_full_path)\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # Save the best model's path for later use\n",
    "    if best_model_state_path is not None:\n",
    "        trial.set_user_attr(\"best_model_state_path\", best_model_state_path)\n",
    "    if best_model_full_path is not None:\n",
    "        trial.set_user_attr(\"best_model_full_path\", best_model_full_path)\n",
    "\n",
    "    # Save only the final model loss curve plot\n",
    "    run_name = f\"BestTrial_H{config['hidden_channels']}_L{config['num_layers']}_HD{config['heads']}_DO{int(config['dropout']*10)}\"\n",
    "    plt.figure()\n",
    "    plt.plot(training_loss, label='Train Loss')\n",
    "    plt.plot(validation_loss, label='Validation Loss')\n",
    "    plt.title(run_name)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(plots_dir, f\"{run_name}_final_loss_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Save loss/acc history to CSV (optional, keep for analysis)\n",
    "    df = pd.DataFrame({\n",
    "        'Epoch': list(range(1, len(training_loss)+1)),\n",
    "        'TrainLoss': training_loss,\n",
    "        'ValLoss': validation_loss,\n",
    "        'TrainAcc': training_acc,\n",
    "        'ValAcc': validation_acc\n",
    "    })\n",
    "    df.to_csv(os.path.join(results_dir, f\"{run_name}.csv\"), index=False)\n",
    "\n",
    "    return max(validation_acc) if validation_acc else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7992855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# with torch.serialization.safe_globals([Data, DataEdgeAttr, DataTensorAttr, GlobalStorage]):\n",
    "#     data_list = torch.load(f\"DatacheckpointNew_Training.pt\", map_location='cpu')\n",
    "\n",
    "# labels = json.load(open(\"label_encoding.json\"))\n",
    "# batch_size = 1\n",
    "\n",
    "# train_split = int(len(data_list) * 0.8)\n",
    "# train_data = data_list[:train_split]\n",
    "# val_data = data_list[train_split:]\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "# in_channels = data_list[0].x.size(1)\n",
    "# num_classes = len(labels)\n",
    "# # num_classes = 4\n",
    "\n",
    "#     # ðŸ”§ Use only one configuration here:\n",
    "# config = {\n",
    "#         'hidden_channels':256,\n",
    "#         'num_layers': 2,\n",
    "#         'heads':8,\n",
    "#         'dropout': 0.2\n",
    "#     }\n",
    "\n",
    "# run_name = f\"SingleRun_H{config['hidden_channels']}_L{config['num_layers']}_HD{config['heads']}_DO{int(config['dropout']*10)}_Updated\"\n",
    "\n",
    "# model_dir = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\GAT-model testing\\\\models\"\n",
    "# results_dir = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\GAT-model testing\\\\results\"\n",
    "# plots_dir = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\GAT-model testing\\\\plots\"\n",
    "\n",
    "# print(f\"\\nðŸš€ Starting {run_name}\")\n",
    "# train_single_config(config, train_loader, val_loader, in_channels, num_classes, run_name, model_dir, results_dir, plots_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f91ec8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # âœ… NodeFormer-style Graph Transformer for Node-Level Classification\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.nn import knn_graph\n",
    "# from torch_scatter import scatter_mean\n",
    "\n",
    "# class NodeFormerLayer(nn.Module):\n",
    "#     def __init__(self, in_dim, out_dim, k=16):\n",
    "#         super().__init__()\n",
    "#         self.k = k\n",
    "#         self.attn_proj = nn.Linear(in_dim, out_dim)\n",
    "#         self.val_proj = nn.Linear(in_dim, out_dim)\n",
    "#         self.out_proj = nn.Linear(out_dim, out_dim)\n",
    "\n",
    "#     def forward(self, x, batch):\n",
    "#         # x: [N, F]  -- node features\n",
    "#         # batch: [N] -- batch IDs\n",
    "\n",
    "#         edge_index = knn_graph(x, self.k, batch=batch, loop=False)\n",
    "#         row, col = edge_index\n",
    "\n",
    "#         # Attention score between i and j\n",
    "#         q = self.attn_proj(x)  # [N, D]\n",
    "#         v = self.val_proj(x)\n",
    "\n",
    "#         attn_score = (q[row] * q[col]).sum(dim=-1) / (q.size(-1) ** 0.5)  # [E]\n",
    "#         attn_score = F.softmax(attn_score, dim=0)\n",
    "\n",
    "#         # Weighted aggregation\n",
    "#         out = attn_score.unsqueeze(-1) * v[col]  # [E, D]\n",
    "#         out = scatter_mean(out, row, dim=0, dim_size=x.size(0))  # [N, D]\n",
    "\n",
    "#         return self.out_proj(out) + x  # Residual\n",
    "\n",
    "\n",
    "# class NodeFormer(nn.Module):\n",
    "#     def __init__(self, in_dim, hidden_dim, out_dim, num_layers=2, k=16):\n",
    "#         super().__init__()\n",
    "#         self.input_proj = nn.Linear(in_dim, hidden_dim)\n",
    "#         self.layers = nn.ModuleList([\n",
    "#             NodeFormerLayer(hidden_dim, hidden_dim, k=k)\n",
    "#             for _ in range(num_layers)\n",
    "#         ])\n",
    "#         self.classifier = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "#     def forward(self, x, batch):\n",
    "#         x = self.input_proj(x)\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x, batch)\n",
    "#         return self.classifier(x)\n",
    "\n",
    "\n",
    "# # # Example usage:\n",
    "# # if __name__ == '__main__':\n",
    "# #     from torch_geometric.datasets import Planetoid\n",
    "# #     from torch_geometric.loader import DataLoader\n",
    "# #     from torch_geometric.utils import to_dense_batch\n",
    "    \n",
    "# #     dataset = Planetoid(root=\"./data\", name=\"Cora\")\n",
    "# #     data = dataset[0]\n",
    "\n",
    "# #     model = NodeFormer(in_dim=dataset.num_node_features, hidden_dim=64, out_dim=dataset.num_classes)\n",
    "# #     optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "# #     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# #     model.train()\n",
    "# #     for epoch in range(100):\n",
    "# #         optimizer.zero_grad()\n",
    "# #         out = model(data.x, batch=torch.zeros_like(data.y))\n",
    "# #         loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "# #         loss.backward()\n",
    "# #         optimizer.step()\n",
    "# #         print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64369c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "56d7ff3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
      "Collecting torch-cluster\n",
      "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_cluster-1.6.3%2Bpt25cu121-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "     ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 0.3/1.6 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 0.3/1.6 MB ? eta -:--:--\n",
      "     ------------------- -------------------- 0.8/1.6 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.6/1.6 MB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from torch-cluster) (1.15.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\user\\anaconda3\\envs\\envgat\\lib\\site-packages (from scipy->torch-cluster) (2.0.1)\n",
      "Installing collected packages: torch-cluster\n",
      "Successfully installed torch-cluster-1.6.3+pt25cu121\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall -y torch-cluster\n",
    "!pip install torch-cluster -f https://data.pyg.org/whl/torch-2.5.1+cu121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b94ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-14 15:16:02,943] A new study created in memory with name: no-name-a3ae3b11-ff0b-4341-9e0c-fb2bbd46cbab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Loss: 1.3842 | Val Loss: 1.3947\n",
      "Epoch 002 | Train Loss: 1.3772 | Val Loss: 1.3921\n",
      "Epoch 002 | Train Loss: 1.3772 | Val Loss: 1.3921\n",
      "Epoch 003 | Train Loss: 1.3711 | Val Loss: 1.3875\n",
      "Epoch 003 | Train Loss: 1.3711 | Val Loss: 1.3875\n",
      "Epoch 004 | Train Loss: 1.3643 | Val Loss: 1.3850\n",
      "Epoch 004 | Train Loss: 1.3643 | Val Loss: 1.3850\n",
      "Epoch 005 | Train Loss: 1.3542 | Val Loss: 1.3814\n",
      "Epoch 005 | Train Loss: 1.3542 | Val Loss: 1.3814\n",
      "Epoch 006 | Train Loss: 1.3413 | Val Loss: 1.3739\n",
      "Epoch 006 | Train Loss: 1.3413 | Val Loss: 1.3739\n",
      "Epoch 007 | Train Loss: 1.3223 | Val Loss: 1.3667\n",
      "Epoch 007 | Train Loss: 1.3223 | Val Loss: 1.3667\n",
      "Epoch 008 | Train Loss: 1.2884 | Val Loss: 1.3547\n",
      "Epoch 008 | Train Loss: 1.2884 | Val Loss: 1.3547\n",
      "Epoch 009 | Train Loss: 1.2500 | Val Loss: 1.3411\n",
      "Epoch 009 | Train Loss: 1.2500 | Val Loss: 1.3411\n",
      "Epoch 010 | Train Loss: 1.2040 | Val Loss: 1.3252\n",
      "Epoch 010 | Train Loss: 1.2040 | Val Loss: 1.3252\n",
      "Epoch 011 | Train Loss: 1.1631 | Val Loss: 1.3104\n",
      "Epoch 011 | Train Loss: 1.1631 | Val Loss: 1.3104\n",
      "Epoch 012 | Train Loss: 1.1149 | Val Loss: 1.2949\n",
      "Epoch 012 | Train Loss: 1.1149 | Val Loss: 1.2949\n",
      "Epoch 013 | Train Loss: 1.0634 | Val Loss: 1.2779\n",
      "Epoch 013 | Train Loss: 1.0634 | Val Loss: 1.2779\n",
      "Epoch 014 | Train Loss: 1.0172 | Val Loss: 1.2614\n",
      "Epoch 014 | Train Loss: 1.0172 | Val Loss: 1.2614\n",
      "Epoch 015 | Train Loss: 0.9712 | Val Loss: 1.2450\n",
      "Epoch 015 | Train Loss: 0.9712 | Val Loss: 1.2450\n",
      "Epoch 016 | Train Loss: 0.9236 | Val Loss: 1.2278\n",
      "Epoch 016 | Train Loss: 0.9236 | Val Loss: 1.2278\n",
      "Epoch 017 | Train Loss: 0.8788 | Val Loss: 1.2131\n",
      "Epoch 017 | Train Loss: 0.8788 | Val Loss: 1.2131\n",
      "Epoch 018 | Train Loss: 0.8505 | Val Loss: 1.1953\n",
      "Epoch 018 | Train Loss: 0.8505 | Val Loss: 1.1953\n",
      "Epoch 019 | Train Loss: 0.8199 | Val Loss: 1.1772\n",
      "Epoch 019 | Train Loss: 0.8199 | Val Loss: 1.1772\n",
      "Epoch 020 | Train Loss: 0.7875 | Val Loss: 1.1642\n",
      "Epoch 020 | Train Loss: 0.7875 | Val Loss: 1.1642\n",
      "Epoch 021 | Train Loss: 0.7613 | Val Loss: 1.1572\n",
      "Epoch 021 | Train Loss: 0.7613 | Val Loss: 1.1572\n",
      "Epoch 022 | Train Loss: 0.7349 | Val Loss: 1.1488\n",
      "Epoch 022 | Train Loss: 0.7349 | Val Loss: 1.1488\n",
      "Epoch 023 | Train Loss: 0.7164 | Val Loss: 1.1355\n",
      "Epoch 023 | Train Loss: 0.7164 | Val Loss: 1.1355\n",
      "Epoch 024 | Train Loss: 0.7037 | Val Loss: 1.1236\n",
      "Epoch 024 | Train Loss: 0.7037 | Val Loss: 1.1236\n",
      "Epoch 025 | Train Loss: 0.6883 | Val Loss: 1.1173\n",
      "Epoch 025 | Train Loss: 0.6883 | Val Loss: 1.1173\n",
      "Epoch 026 | Train Loss: 0.6813 | Val Loss: 1.1082\n",
      "Epoch 026 | Train Loss: 0.6813 | Val Loss: 1.1082\n",
      "Epoch 027 | Train Loss: 0.6721 | Val Loss: 1.1054\n",
      "Epoch 027 | Train Loss: 0.6721 | Val Loss: 1.1054\n",
      "Epoch 028 | Train Loss: 0.6590 | Val Loss: 1.0982\n",
      "Epoch 028 | Train Loss: 0.6590 | Val Loss: 1.0982\n",
      "Epoch 029 | Train Loss: 0.6542 | Val Loss: 1.0941\n",
      "Epoch 029 | Train Loss: 0.6542 | Val Loss: 1.0941\n",
      "Epoch 030 | Train Loss: 0.6466 | Val Loss: 1.0829\n",
      "Epoch 030 | Train Loss: 0.6466 | Val Loss: 1.0829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-07-14 15:16:14,717] Trial 0 failed with parameters: {'hidden_channels': 64, 'num_layers': 3, 'heads': 4, 'dropout': 0.06745096027957437, 'hidden_dim': 256} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_28788\\1925102268.py\", line 99, in objective\n",
      "    out = model(data.x, data.edge_index, edge_weight=data.edge_attr)\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch_geometric\\nn\\models\\basic_gnn.py\", line 254, in forward\n",
      "    x = conv(x, edge_index, edge_attr=edge_attr)\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch_geometric\\nn\\conv\\gatv2_conv.py\", line 325, in forward\n",
      "    alpha = self.edge_updater(edge_index, x=(x_l, x_r),\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\torch_geometric.nn.conv.gatv2_conv_GATv2Conv_edge_updater_w14_91fn.py\", line 176, in edge_updater\n",
      "    out = self.edge_update(\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch_geometric\\nn\\conv\\gatv2_conv.py\", line 370, in edge_update\n",
      "    alpha = softmax(alpha, index, ptr, dim_size)\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch_geometric\\utils\\_softmax.py\", line 76, in softmax\n",
      "    out = src - src_max.index_select(dim, index)\n",
      "KeyboardInterrupt\n",
      "[W 2025-07-14 15:16:14,793] Trial 0 failed with value None.\n",
      "[W 2025-07-14 15:16:14,793] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 031 | Train Loss: 0.6383 | Val Loss: 1.0771\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# print(\"  Accuracy:\", study.best_trial.value)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\optuna\\study\\study.py:489\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    389\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 64\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    249\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    252\u001b[0m ):\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[64], line 99\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     97\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m#  Move batch to GPU\u001b[39;00m\n\u001b[0;32m     98\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 99\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m    101\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch_geometric\\nn\\models\\basic_gnn.py:254\u001b[0m, in \u001b[0;36mBasicGNN.forward\u001b[1;34m(self, x, edge_index, edge_weight, edge_attr, batch, batch_size, num_sampled_nodes_per_hop, num_sampled_edges_per_hop)\u001b[0m\n\u001b[0;32m    252\u001b[0m     x \u001b[38;5;241m=\u001b[39m conv(x, edge_index, edge_weight\u001b[38;5;241m=\u001b[39medge_weight)\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_edge_attr:\n\u001b[1;32m--> 254\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     x \u001b[38;5;241m=\u001b[39m conv(x, edge_index)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch_geometric\\nn\\conv\\gatv2_conv.py:325\u001b[0m, in \u001b[0;36mGATv2Conv.forward\u001b[1;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe usage of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_attr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd_self_loops\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimultaneously is currently not yet supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseTensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m form\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# edge_updater_type: (x: PairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_updater\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_r\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m                          \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: PairTensor, alpha: Tensor)\u001b[39;00m\n\u001b[0;32m    329\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39m(x_l, x_r), alpha\u001b[38;5;241m=\u001b[39malpha)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\torch_geometric.nn.conv.gatv2_conv_GATv2Conv_edge_updater_w14_91fn.py:176\u001b[0m, in \u001b[0;36medge_updater\u001b[1;34m(self, edge_index, x, edge_attr, size)\u001b[0m\n\u001b[0;32m    166\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[0;32m    167\u001b[0m                 x_j\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_j\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    168\u001b[0m                 x_i\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_i\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    173\u001b[0m             )\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# End Edge Update Forward Pre Hook #########################################\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_j\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_i\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# Begin Edge Update Forward Hook ###########################################\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch_geometric\\nn\\conv\\gatv2_conv.py:370\u001b[0m, in \u001b[0;36mGATv2Conv.edge_update\u001b[1;34m(self, x_j, x_i, edge_attr, index, ptr, dim_size)\u001b[0m\n\u001b[0;32m    368\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnegative_slope)\n\u001b[0;32m    369\u001b[0m alpha \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 370\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m alpha \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(alpha, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m alpha\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ENVGAT\\lib\\site-packages\\torch_geometric\\utils\\_softmax.py:76\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(src, index, ptr, num_nodes, dim)\u001b[0m\n\u001b[0;32m     74\u001b[0m N \u001b[38;5;241m=\u001b[39m maybe_num_nodes(index, num_nodes)\n\u001b[0;32m     75\u001b[0m src_max \u001b[38;5;241m=\u001b[39m scatter(src\u001b[38;5;241m.\u001b[39mdetach(), index, dim, dim_size\u001b[38;5;241m=\u001b[39mN, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 76\u001b[0m out \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m-\u001b[39m \u001b[43msrc_max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mexp()\n\u001b[0;32m     78\u001b[0m out_sum \u001b[38;5;241m=\u001b[39m scatter(out, index, dim, dim_size\u001b[38;5;241m=\u001b[39mN, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-16\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print(\"Best Trial:\")\n",
    "# print(\"  Accuracy:\", study.best_trial.value)\n",
    "print(\"  Params:\")\n",
    "for k, v in study.best_trial.params.items():\n",
    "    print(f\"    {k}: {v}\")\n",
    "\n",
    "# === No need to retrain: Best model is already saved during Optuna search ===\n",
    "best_trial = study.best_trial\n",
    "print(\"Best model state_dict saved at:\", best_trial.user_attrs[\"best_model_state_path\"])\n",
    "print(\"Best full model saved at:\", best_trial.user_attrs[\"best_model_full_path\"])\n",
    "\n",
    "# Print best train and validation accuracy from CSV\n",
    "run_name = f\"BestTrial_H{best_trial.params['hidden_channels']}_L{best_trial.params['num_layers']}_HD{best_trial.params['heads']}_DO{int(best_trial.params['dropout']*10)}\"\n",
    "csv_path = os.path.join(\"results\", f\"{run_name}.csv\")\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    best_train_acc = df['TrainAcc'].max() if 'TrainAcc' in df else None\n",
    "    best_val_acc = df['ValAcc'].max() if 'ValAcc' in df else None\n",
    "    print(f\"Best Training Accuracy: {best_train_acc:.4f}\" if best_train_acc is not None else \"Best Training Accuracy: N/A\")\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.4f}\" if best_val_acc is not None else \"Best Validation Accuracy: N/A\")\n",
    "else:\n",
    "    print(f\"Could not find CSV file for best trial at {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2cfda6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8724647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save best params to JSON\n",
    "# best_params_path = os.path.join(model_dir, \"best_params.json\")\n",
    "# with open(best_params_path, \"w\") as f:\n",
    "#     json.dump(study.best_trial.params, f, indent=4)\n",
    "\n",
    "# # ----------------- FINAL MODEL TRAINING ----------------------\n",
    "\n",
    "# # Build model with best params\n",
    "# best_params = study.best_trial.params\n",
    "\n",
    "# final_model = GAT(\n",
    "#     in_channels=in_channels,\n",
    "#     hidden_channels=best_params['hidden_channels'],\n",
    "#     num_layers=best_params['num_layers'],\n",
    "#     out_channels=num_classes,\n",
    "#     dropout=best_params['dropout'],\n",
    "#     heads=best_params['heads'],\n",
    "#     v2=True,\n",
    "#     edge_dim=1,\n",
    "#     jk='lstm'\n",
    "# ).to(device)\n",
    "# # final_model = NodeFormer(\n",
    "# #     in_dim=in_channels,\n",
    "# #     hidden_dim=best_params['hidden_dim'],\n",
    "# #     out_dim=num_classes,\n",
    "# #     num_layers=best_params['num_layers'],\n",
    "# #     k=best_params['k']\n",
    "# # ).to(device)\n",
    "# # Loss and optimizer setup\n",
    "# all_labels = torch.cat([data.y for data in train_loader.dataset])\n",
    "# class_counts = torch.bincount(all_labels, minlength=num_classes)\n",
    "# class_weights = 1.0 / (class_counts.float() + 1e-6)\n",
    "# class_weights = class_weights / class_weights.sum()\n",
    "# class_weights = class_weights.to(device)\n",
    "\n",
    "# criterion = CrossEntropyLoss(weight=class_weights)\n",
    "# optimizer = torch.optim.Adam(final_model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "\n",
    "# # Train final model\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# best_val_loss = float('inf')\n",
    "# best_model_path = os.path.join(model_dir, \"GAT_full_model_best_001.pt\")\n",
    "# for epoch in range(500):\n",
    "#     final_model.train()\n",
    "#     train_loss = 0\n",
    "#     for data in train_loader:\n",
    "#         data = data.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         out = final_model(data.x, data.edge_index, edge_weight=data.edge_attr)\n",
    "#         # out = final_model(data.x, batch=None)\n",
    "#         loss = criterion(out, data.y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.item()\n",
    "#     avg_train_loss = train_loss / len(train_loader)\n",
    "#     train_losses.append(avg_train_loss)\n",
    "\n",
    "#     # Validation loss\n",
    "#     final_model.eval()\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data in val_loader:\n",
    "#             data = data.to(device)\n",
    "#             out = final_model(data.x, data.edge_index, edge_weight=data.edge_attr)\n",
    "#             loss = criterion(out, data.y)\n",
    "#             val_loss += loss.item()\n",
    "#     avg_val_loss = val_loss / len(val_loader)\n",
    "#     val_losses.append(avg_val_loss)\n",
    "\n",
    "#     # Save best model (lowest val loss)\n",
    "#     if avg_val_loss < best_val_loss:\n",
    "#         best_val_loss = avg_val_loss\n",
    "#         torch.save(final_model.state_dict(), best_model_path)\n",
    "\n",
    "#     print(f\"[FINAL TRAIN] Epoch {epoch+1:03d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# # ----------------- SAVE FULL MODEL ----------------------\n",
    "\n",
    "# full_model_path = os.path.join(model_dir, \"GAT_full_model_2.pt\")\n",
    "# torch.save(final_model, full_model_path)\n",
    "# print(f\"âœ… Full model saved to {full_model_path}\")\n",
    "# print(f\"âœ… Best model (lowest val loss) saved to {best_model_path}\")\n",
    "\n",
    "# # ----------------- PLOT TRAIN/VAL LOSS ----------------------\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(8,5))\n",
    "# plt.plot(train_losses, label='Train Loss')\n",
    "# plt.plot(val_losses, label='Validation Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Final Model Training and Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(os.path.join(plots_dir, \"final_train_val_loss.png\"))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59aa8feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13a05235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of True vs Predicted Labels:\n",
      "Sample 1: True label = KEY | Predicted label = NON_RELATED\n",
      "Sample 2: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 3: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 4: True label = KEY | Predicted label = NON_RELATED\n",
      "Sample 5: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 6: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 7: True label = KEY | Predicted label = KEY\n",
      "Sample 8: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 9: True label = NON_RELATED | Predicted label = OTHER_KEY\n",
      "Sample 10: True label = NON_RELATED | Predicted label = OTHER_KEY\n",
      "Sample 11: True label = KEY | Predicted label = NON_RELATED\n",
      "Sample 12: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 13: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 14: True label = NON_RELATED | Predicted label = OTHER_KEY\n",
      "Sample 15: True label = KEY | Predicted label = KEY\n",
      "Sample 16: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 17: True label = NON_RELATED | Predicted label = OTHER_KEY\n",
      "Sample 18: True label = NON_RELATED | Predicted label = OTHER_KEY\n",
      "Sample 19: True label = KEY | Predicted label = KEY\n",
      "Sample 20: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 21: True label = NON_RELATED | Predicted label = OTHER_KEY\n",
      "Sample 22: True label = NON_RELATED | Predicted label = OTHER_KEY\n",
      "Sample 23: True label = KEY | Predicted label = KEY\n",
      "Sample 24: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 25: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 26: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 27: True label = KEY | Predicted label = KEY\n",
      "Sample 28: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 29: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 30: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 31: True label = KEY | Predicted label = NON_RELATED\n",
      "Sample 32: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 33: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 34: True label = NON_RELATED | Predicted label = OTHER_KEY\n",
      "Sample 35: True label = KEY | Predicted label = KEY\n",
      "Sample 36: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 37: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 38: True label = NON_RELATED | Predicted label = OTHER_KEY\n",
      "Sample 39: True label = KEY | Predicted label = KEY\n",
      "Sample 40: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 41: True label = NON_RELATED | Predicted label = OTHER_KEY\n",
      "Sample 42: True label = NON_RELATED | Predicted label = OTHER_KEY\n",
      "Sample 43: True label = KEY | Predicted label = KEY\n",
      "Sample 44: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 45: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 46: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 47: True label = KEY | Predicted label = NON_RELATED\n",
      "Sample 48: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 49: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 50: True label = KEY | Predicted label = KEY\n",
      "Sample 51: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 52: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 53: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 54: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 55: True label = KEY | Predicted label = KEY\n",
      "Sample 56: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 57: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 58: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 59: True label = KEY | Predicted label = KEY\n",
      "Sample 60: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 61: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 62: True label = NON_RELATED | Predicted label = OTHER_KEY\n",
      "Sample 63: True label = KEY | Predicted label = KEY\n",
      "Sample 64: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 65: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 66: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 67: True label = KEY | Predicted label = KEY\n",
      "Sample 68: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 69: True label = NON_RELATED | Predicted label = OTHER_KEY\n",
      "Sample 70: True label = NON_RELATED | Predicted label = OTHER_KEY\n",
      "Sample 71: True label = KEY | Predicted label = KEY\n",
      "Sample 72: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 73: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 74: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 75: True label = KEY | Predicted label = NON_RELATED\n",
      "Sample 76: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 77: True label = NON_RELATED | Predicted label = NON_RELATED\n",
      "Sample 78: True label = NON_RELATED | Predicted label = OTHER_KEY\n",
      "Sample 79: True label = KEY | Predicted label = KEY\n",
      "Sample 80: True label = NON_RELATED | Predicted label = VALUE\n",
      "Sample 81: True label = NON_RELATED | Predicted label = NON_RELATED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_28788\\3802918630.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\GAT-model testing\\\\GAT-test\\\\models\\\\BestTrial_H128_L3_HD1_DO3_best_full.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Load the saved full model\n",
    "model = torch.load(\"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\GAT-model testing\\\\GAT-test\\\\models\\\\BestTrial_H128_L3_HD1_DO3_best_full.pt\")\n",
    "model.eval()\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load label decoder (to map index -> class name)\n",
    "label_mapping = json.load(open(\"label_encoding.json\"))\n",
    "index_to_label = {v: k for k, v in label_mapping.items()}  # reverse mapping\n",
    "\n",
    "# Load the data you want to predict on\n",
    "data_list = torch.load(\"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\GAT-model testing\\\\GAT-test\\\\data\\\\background_verification\\\\datacheckpoint_1.pt\", map_location='cuda', weights_only=False)\n",
    "test_loader = DataLoader(data_list, batch_size=1, shuffle=False)\n",
    "\n",
    "# Predict on each sample\n",
    "predictions = []\n",
    "true_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, edge_weight=data.edge_attr)\n",
    "        pred = out.argmax(dim=1).cpu().numpy()\n",
    "        labels = [index_to_label[int(p)] for p in pred]\n",
    "        predictions.extend(labels)\n",
    "        # Collect true labels if available\n",
    "        if hasattr(data, 'y'):\n",
    "            true_labels.extend([index_to_label[int(y)] for y in data.y.cpu().numpy()])\n",
    "\n",
    "# Only print comparison between true and predicted labels if available\n",
    "if true_labels:\n",
    "    print(\"Comparison of True vs Predicted Labels:\")\n",
    "    for i, (true_label, pred_label) in enumerate(zip(true_labels, predictions)):\n",
    "        print(f\"Sample {i+1}: True label = {true_label} | Predicted label = {pred_label}\")\n",
    "else:\n",
    "    print(\"No true labels found in test data. Cannot compare.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ebae311e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[19, 18], edge_index=[2, 13], edge_attr=[13, 1], y=[19])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = torch.load(\"datacheckpoint_01 (1).pt\", map_location='cuda', weights_only=False)\n",
    "data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64a1eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Compute accuracy if true labels are available\n",
    "if true_labels:\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Test Classification Accuracy: {accuracy:.4f}\")\n",
    "else:\n",
    "    print(\"No true labels found in test data. Accuracy cannot be computed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENVGAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
